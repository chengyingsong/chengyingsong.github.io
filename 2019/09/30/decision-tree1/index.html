<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="ML,算法,">










<meta name="description" content="决策树 （Decision tree)【统学笔记】概述决策树学习通常包括3个步骤：  特征选择 决策树的生成 决策树的修剪  主要算法： ID3,  C4.5 ,  CART算法 决策树的节点分为两种： 内部节点和叶结点。内部节点表示一个特征或者属性，叶节点表示一个类。  决策树是利用if-then的集合将样本空间划为为互不相交的单元（cell）或者区域(region)，并在每个单元定义一个类的概">
<meta name="keywords" content="ML,算法">
<meta property="og:type" content="article">
<meta property="og:title" content="决策树 （Decision tree)【统学笔记】">
<meta property="og:url" content="http://yoursite.com/2019/09/30/decision-tree1/index.html">
<meta property="og:site_name" content="柠檬笔记">
<meta property="og:description" content="决策树 （Decision tree)【统学笔记】概述决策树学习通常包括3个步骤：  特征选择 决策树的生成 决策树的修剪  主要算法： ID3,  C4.5 ,  CART算法 决策树的节点分为两种： 内部节点和叶结点。内部节点表示一个特征或者属性，叶节点表示一个类。  决策树是利用if-then的集合将样本空间划为为互不相交的单元（cell）或者区域(region)，并在每个单元定义一个类的概">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/2019/09/30/decision-tree1/decesion_tree.png">
<meta property="og:updated_time" content="2019-10-01T06:57:16.825Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="决策树 （Decision tree)【统学笔记】">
<meta name="twitter:description" content="决策树 （Decision tree)【统学笔记】概述决策树学习通常包括3个步骤：  特征选择 决策树的生成 决策树的修剪  主要算法： ID3,  C4.5 ,  CART算法 决策树的节点分为两种： 内部节点和叶结点。内部节点表示一个特征或者属性，叶节点表示一个类。  决策树是利用if-then的集合将样本空间划为为互不相交的单元（cell）或者区域(region)，并在每个单元定义一个类的概">
<meta name="twitter:image" content="http://yoursite.com/2019/09/30/decision-tree1/decesion_tree.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/09/30/decision-tree1/">





  <title>决策树 （Decision tree)【统学笔记】 | 柠檬笔记</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">柠檬笔记</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">柠檬树上柠檬果，柠檬树下只有我</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            日志
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/09/30/decision-tree1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="莫久">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/title.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="柠檬笔记">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">决策树 （Decision tree)【统学笔记】</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-09-30T23:49:56+08:00">
                2019-09-30
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="决策树-（Decision-tree-【统学笔记】"><a href="#决策树-（Decision-tree-【统学笔记】" class="headerlink" title="决策树 （Decision tree)【统学笔记】"></a>决策树 （Decision tree)【统学笔记】</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>决策树学习通常包括3个步骤：</p>
<ul>
<li>特征选择</li>
<li>决策树的生成</li>
<li>决策树的修剪</li>
</ul>
<p>主要算法： ID3,  C4.5 ,  CART算法</p>
<p>决策树的节点分为两种： 内部节点和叶结点。内部节点表示一个特征或者属性，叶节点表示一个类。</p>
<blockquote>
<p>决策树是利用if-then的集合将样本空间划为为互不相交的单元（cell）或者区域(region)，并在每个单元定义一个类的概率分布就构成了一个条件概率分布</p>
</blockquote>
<h2 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h2><p>特征选择在于选取对于训练数据具有分类能力的特征，这样可以提高决策树学习的效率。通常特征选择的准则是<strong>信息增益</strong>或者<strong>信息增益比</strong></p>
<h3 id="熵的定义"><a href="#熵的定义" class="headerlink" title="熵的定义"></a>熵的定义</h3><p>==<strong>熵（entropy）</strong>是表示随机变量不确定性的度量。==</p>
<p>设X是一个取有限个值的离散随机变量，其概率分布为 $P(X = x_i) = p_i$   i = 1,2,3..n</p>
<p>则随机变量X的熵定义为$H(x) = - \sum_{i =1} ^n p_i logp_i$ </p>
<p>可以看出，熵的定义与X的取值无关，只和P的分布有关，所以也可以写为$H(p) = - \sum_{i =1} ^n p_i logp_i$</p>
<p>==熵越大，随机变量的不确定性就越大== </p>
<p>设有随机变量（X,Y),其联合概率分布为  $P(X = x_i,Y = y_j) =p_{ij}$ ,i =1,2,..n,j=1,2..m</p>
<p><strong>条件熵H(Y|X)</strong> 表示在已知随机变量X的条件下的随机变量Y的不确定性。 定义为X给定条件下Y的条件概率分布的熵对X的数学期望</p>
<script type="math/tex; mode=display">
H(Y|X) = \sum_{i=1}^np_iH(Y|X =x_i)</script><blockquote>
<p>$p_i = P(X =x_i)$,  i =  1,2,..n</p>
</blockquote>
<p>当熵和条件熵中的概率由数据估计（特别是极大似然估计）得到时，所对应的熵和条件熵被称为经验熵（empirical entropy)和经验条件熵（empirical conditional entropy)。特别： 如果有0概率 ，  0 log0 = 0</p>
<h3 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h3><p>==信息增益（information gain)表示得知特征X的信息而使得类Y的信息不确定性减少的程度== </p>
<p><strong>信息增益</strong> ： 特征A对训练数据集D的信息增益g(D,A),定义为集合D的经验熵 H(D)与特征A给定条件下D的经验条件熵H(D|A)之差，即<strong>g(D|A) = H(D) - H(D|A)</strong></p>
<h4 id="信息增益计算算法"><a href="#信息增益计算算法" class="headerlink" title="信息增益计算算法"></a>信息增益计算算法</h4><ol>
<li>计算数据集D的经验熵H(D)</li>
</ol>
<script type="math/tex; mode=display">
H(D) = -\sum_{k=1}^K\frac{|C_k|}{|D|}log_2\frac{|C_k|}{D}</script><ol>
<li>计算特征A对数据集D的经验条件熵H(D|A)</li>
</ol>
<script type="math/tex; mode=display">
H(D|A) = \sum_{i=1}^n\frac{|D_i|}{|D|}H(D_i)</script><ol>
<li>计算信息增益</li>
</ol>
<script type="math/tex; mode=display">
g(D|A)  =H(D) - H(D|A)</script><h3 id="信息增益比"><a href="#信息增益比" class="headerlink" title="信息增益比"></a>信息增益比</h3><p>信息增益值的大小是相对于训练数据集而言的，并没有绝对意义，在分类问题困难时，也就是在训练数据集的经验熵大的时候，信息增益值会偏大。反之，偏小。使用信息增益比（information gain ratio）可以对这一问题进行校正，这是特征选择的另一准则</p>
<p><strong>信息增益比</strong> 的定义是：特征A对训练数据集D的信息增益比$g_k(D,A)$定义为其信息增益g(D,A)与训练数据集D的经验熵H(D)之比：</p>
<script type="math/tex; mode=display">
g_R(D,A) = \frac {g(D，A)}{H(D)}</script><h2 id="决策树的生成"><a href="#决策树的生成" class="headerlink" title="决策树的生成"></a>决策树的生成</h2><h3 id="ID3算法"><a href="#ID3算法" class="headerlink" title="ID3算法"></a>ID3算法</h3><p>ID3算法的核心是在决策树各个结点上应用信息增益准则选择特征，递归的构建决策树。具体方法是：从根节点开始，<strong>对节点计算所有可能特征的信息增益</strong>，选择信息增益最大的特征作为节点的特征，由该特征的不同取值建立子结点，再对子结点递归的调用以上方法，构建决策树。<strong>直到所有特征的信息增益都很小（阈值）或者没有特征可以选择为止</strong>，最后得到一个决策树。</p>
<blockquote>
<p>ID3相当于用极大似然法进行概率模型的选择</p>
</blockquote>
<p><strong>算法描述</strong> ：</p>
<p>输入：训练数据集D，特征集A，阈值$\epsilon$</p>
<p>输出：决策树T</p>
<ol>
<li>如果D中的所有实例属于同一类$C_k$，则T为单结点树，并将类$C_k$作为该结点的标记【递归边界1】</li>
<li>若A 为空集，则T为单结点树，并将D中实例数最大的类作为该结点的类标记，返回T；【递归边界2】</li>
<li>否则，计算A中各个特征的对D的信息增益，选择信息增益最大的特征$A_g$;</li>
<li>如果$A_g$ 的信息增益小于阈值$\epsilon$ ,则置T为单结点树，并将D中实例数最大的类$C_k$作为标记，构建子结点，由结点及其子结点构成树T，返回T。【递归边界3，避免过拟合】</li>
<li>否则，对$A_g$的每一个可能值$a_i$,依$A_g  =a_i$将D分割为若干非空子集，将$D_i$中最多的类作为标记，构建子结点，由结点及其子结点构成树T，返回T</li>
<li>对第i个子结点，以$D_i$为训练集，以A-{$A_g$}为特征集，递归的调用，1~5.得到子树并返回</li>
</ol>
<h3 id="C4-5算法"><a href="#C4-5算法" class="headerlink" title="C4.5算法"></a>C4.5算法</h3><p>C4.5和ID3的不同之处就是在生成的过程中，不使用信息增益，而使用信息增益比来构建决策树。</p>
<h3 id="CART算法（classification-and-regression-tree"><a href="#CART算法（classification-and-regression-tree" class="headerlink" title="CART算法（classification and regression tree)"></a>CART算法（classification and regression tree)</h3><h4 id="CART-分类树"><a href="#CART-分类树" class="headerlink" title="CART 分类树"></a>CART 分类树</h4><p>CART分类树的生成，使用GIni指数，（基尼指数）来选择最优特征，其他的基本相同，【同时是生成二叉树】</p>
<p><strong>基尼指数</strong> ：在分类问题中，假设有K个类，样本点属于第k类的概率为$p_k$,则概率分布的基尼指数定义为：</p>
<script type="math/tex; mode=display">
Gini(p) = \sum_{k =1}^Kp_k(1-p_k) = 1 - \sum_{k=1}^Kp_k^2</script><blockquote>
<p>递归边界可以设置为，结点中样本个数小于阈值或者样本集的基尼指数小于预定阈值（样本基本属于同一类），或者没有更多的特征。</p>
</blockquote>
<h4 id="CART回归树"><a href="#CART回归树" class="headerlink" title="CART回归树"></a>CART回归树</h4><p>假设X和Y分别为输入和输出变量，并且Y是连续变量，给定训练数据集 D= {$(x_1,y_1),(x_2,y_2),…(x_N,y_N)$}</p>
<p>一个回归树对应着输入空间的一个划分以及输出单元上的输出值，将输入空间划分为M个单元$R_1,R_2,..R_M$,并且在每个单元$R_m$上有一个固定的输出值 $c_m$，于是回归树模型可以表示为</p>
<script type="math/tex; mode=display">
f(x) = \sum_{m=1}^M c_mI(x\in R_m)</script><p>当输入空间的划分确定时，可以用平方误差$\sum_{x_i \in R_m}(y_i - f(x_i))^2$来表示回归树对于训练数据的预测误差，用平方误差最小的准则求解每个单元上的最优输出值。易知，单元$R_m$上的$c_m$的最优值是所有输入实例对应的输出的均值。</p>
<script type="math/tex; mode=display">
c_m' = ave(y_i|x_i\in R_m)</script><blockquote>
<p>分类树是输出叶结点中实例数最多的类，而回归树是输出所有实例对应输出的均值</p>
</blockquote>
<p>对输入空间进行划分。采用启发式方法，选择第j个变量$x^{(j)}$和它所取的值s，作为切分变量和切分点，并把输入空间划分为两个区域。然后就是要寻找最优切分变量和最优切分点。也就是求解</p>
<script type="math/tex; mode=display">
min_{j,s}[min_{c_1}\sum_{x_i \in R_1(j,s)}(y_i - c_1)^2 + min_{c_2}\sum_{x_i \in R_2(j,s)}(y_i - c_2)^2]</script><blockquote>
<p>也就是最小化划分之后的对于每个划分子空间内的平方误差，以选择划分之后误差最小的划分</p>
</blockquote>
<p><strong>回归树生成算法</strong> </p>
<p>输入：训练数据集D；</p>
<p>输出：回归树 f(x).</p>
<p>在训练数据集所在的输入空间中，递归的将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树。</p>
<ol>
<li>选择最优切分变量j和切分点s，【按照上面的公式】遍历j，对固定的切分变量j扫描切分点s,选择使式达到最小的值的对（j,s)</li>
<li>用选定的对（j,s)划分区域并决定相应的输出值：</li>
</ol>
<script type="math/tex; mode=display">
R_1(j,s) = \{x|x_j <= s\},R_2(j,s) = \{x|x_j>s\}</script><script type="math/tex; mode=display">
c_m' = \frac 1 N_m \sum_{x_i \in R_m(j,s)}y_i,x\in R_m,,m=1,2</script><ol>
<li>继续对两个子区域调用步骤1，2，直至满足停止条件</li>
<li>将输入空间划分为M个区域R1，R2.。。Rm,生成决策树。</li>
</ol>
<h2 id="剪枝"><a href="#剪枝" class="headerlink" title="剪枝"></a>剪枝</h2><p>在决策树学习中将已生成的树进行简化的过程称为剪枝，具体的，剪枝从已生成的树上裁掉一些子树或者叶结点，并将其根节点或者父结点作为新的叶结点，从而简化分类树模型。避免过拟合。</p>
<h3 id="后剪枝"><a href="#后剪枝" class="headerlink" title="后剪枝"></a>后剪枝</h3><p>通过极小化决策树整体的损失函数或者代价函数来实现。设树T的叶结点个数为|T|，t是树T的结点，该叶结点有$N_t$个样本点，其中k类的样本点有$N_{tk}$个，$H_t(T)$为叶结点t上的经验熵，$\alpha&gt;=0$为参数，则损失函数定义为：</p>
<script type="math/tex; mode=display">
C_{\alpha}(T) = \sum_{t=1}^{|T|}N_tH_t(T)+ \alpha|T|</script><p>将第一项记作</p>
<script type="math/tex; mode=display">
C(T)=\sum_{t=1}^{|T|}N_tH_t(T) =  - \sum_{t=1}^{|T|}\sum_{k=1}^KN_{tk}log \frac {N_{tk}} {N_t}</script><p><strong>剪枝算法</strong> </p>
<p>输入：生成算法产生的整个树T，参数$\alpha$</p>
<p>输出：修剪后的子树$T_{\alpha}$ </p>
<ol>
<li>计算每个结点的经验熵</li>
<li>递归的从树的叶结点开始向上回缩，如果回缩之前的树为$T_B$和$T_A$,其对应的损失函数分别是$C_{\alpha}(T_B)$和$C_{\alpha}(T_A)$,如果 </li>
</ol>
<script type="math/tex; mode=display">
C_{\alpha}(T_A) <=C_{\alpha}(T_B)</script><p>就进行剪枝</p>
<ol>
<li>返回2，直到不能继续为止，得到损失函数最小的子树T</li>
</ol>
<blockquote>
<p>决策树的剪枝算法可以由动态规划算法实现</p>
</blockquote>
<p><strong>CART剪枝</strong> </p>
<p>CART剪枝算法由两步组成：首先从生成算法产生的决策树$T_0$底端开始不断剪枝，直到根节点，形成一个子树序列。然后通过交叉验证法在独立的验证数据集上对子树序列进行测试，从中选择最优子树</p>
<ol>
<li>剪枝，形成一个子树序列</li>
</ol>
<p>可以用递归的方法对树进行剪枝，将$\alpha$从小增大，产生一系列的区间$[\alpha_i,\alpha_{i+1}]$,i = 0,1,..n;剪枝得到的子树序列对应着区间$\alpha \in [\alpha_i,\alpha_{i+1}),i=0,1,..n$ ,的最优子树序列，序列中的子树是嵌套的。</p>
<p>具体为，从整体树开始剪枝，对树内任意内部结点t，</p>
<script type="math/tex; mode=display">
g(t) =  \frac {C(t) - C(T_t)} {|T_t| -1}</script><p>它表示剪枝后整体损失函数减少的程度，在T0中剪去g(t)最小的T，将得到的子树作为 T1.同时将最小的g(t)设为$\alpha_1$,$T_1$为区间$[\alpha_1,\alpha_2)$的最优子树</p>
<ol>
<li>在剪枝得到的子树序列$T_0,T_1,..T_n$中通过交叉验证选取最优子树$T_{\alpha}$</li>
</ol>
<p><img src="/2019/09/30/decision-tree1/decesion_tree.png" alt="img"></p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/ML/" rel="tag"># ML</a>
          
            <a href="/tags/算法/" rel="tag"># 算法</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/09/15/baidu-AI/" rel="next" title="baidu_AI">
                <i class="fa fa-chevron-left"></i> baidu_AI
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/title.jpg" alt="莫久">
            
              <p class="site-author-name" itemprop="name">莫久</p>
              <p class="site-description motion-element" itemprop="description">学习笔记</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">11</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">8</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">15</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#决策树-（Decision-tree-【统学笔记】"><span class="nav-number">1.</span> <span class="nav-text">决策树 （Decision tree)【统学笔记】</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#概述"><span class="nav-number">1.1.</span> <span class="nav-text">概述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#特征选择"><span class="nav-number">1.2.</span> <span class="nav-text">特征选择</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#熵的定义"><span class="nav-number">1.2.1.</span> <span class="nav-text">熵的定义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#信息增益"><span class="nav-number">1.2.2.</span> <span class="nav-text">信息增益</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#信息增益计算算法"><span class="nav-number">1.2.2.1.</span> <span class="nav-text">信息增益计算算法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#信息增益比"><span class="nav-number">1.2.3.</span> <span class="nav-text">信息增益比</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#决策树的生成"><span class="nav-number">1.3.</span> <span class="nav-text">决策树的生成</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#ID3算法"><span class="nav-number">1.3.1.</span> <span class="nav-text">ID3算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#C4-5算法"><span class="nav-number">1.3.2.</span> <span class="nav-text">C4.5算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CART算法（classification-and-regression-tree"><span class="nav-number">1.3.3.</span> <span class="nav-text">CART算法（classification and regression tree)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#CART-分类树"><span class="nav-number">1.3.3.1.</span> <span class="nav-text">CART 分类树</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#CART回归树"><span class="nav-number">1.3.3.2.</span> <span class="nav-text">CART回归树</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#剪枝"><span class="nav-number">1.4.</span> <span class="nav-text">剪枝</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#后剪枝"><span class="nav-number">1.4.1.</span> <span class="nav-text">后剪枝</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">莫久</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

</body>
</html>
